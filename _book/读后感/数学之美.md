# 数学之美

读后感： 万事万物都是有相似之处的，还有它的合理之处

- 
- 简单性 格模块化市软件工程的基石， 分布式和容错性是互联网的生命
- 
- 信息的冗余是信息安全的保障
- 语言的数据
- 
- 罗马人的解码规则是加减法
- 中国人的解码规则是乘法
- 
- 任何一种语言都是一种编码方式，而语言的语法规则是解编码的算法。
- 
- 人工智能靠的是统计
- 
- 计算机高级程序语言都可以概括成上下文无关的文法
- 
- 自然语言中产生了词义和上下文相关的特性
- 程序语言人为设计、便于计算机编码的上下文无关文法
- 
- 
- 大数定力 只要统计量足够， 相对频率就等于概率
- 
- 假定任意一个词出现的频率只同它前面的词有关， 马尔可夫假设
- 
- 公式对应的统计模型是二元统计模型
- 
- 
- 利用专业的语料库，只要数据w, w-1这对词在统计的文本中前后相邻出现了多少次， 以及w-1本身在同样的文本中出现了多少次， 然后利用两个书分别除以语料库的大小#，即可得到这些词或者二元组的相对频率。
- 
- 古德-图灵估计 解决小概率 或零概率问题
- 
- 
- 马尔可夫链
- 隐含马尔可夫链： 任一时刻t的状态st是不可见的。
- 维特比算法
- p(s1,s2,s3,s4 | o1,o2,o.3)
- 
- p(st|st-1) st-1进入当前状态st的概率P(st | st-1) 也被称为转移概率
- 每个状态St产生相应输出符号
- 
- P(ot | st) = p(ot, st)/ p(st)
- 
- 一条信息的信息量与其不确定醒有着直接的关系。
- 
- 
- 一比特是一位二进制
- 
- 一字节就是8比特
- 
- log32 =5
- 信息熵 h = -(p1 * logp1 + p2* logp2 + p3* logp3)
- 
- 
- 
- 互信息 两个随机事件“相关性”的量化度量

i(x; y)

解决 词义的二义性就是互信息 找出相关的上下文

人工分词对其影响超过了机器

基本词表和复合词表 做分词切分

分词的准确性 可以分成两类 1. 错误 2. 颗粒度不一致

错误又分为两类1. 越界行错误 2. 覆盖型错误

颗粒度错误 人工分词的不一致性

语言模型需要知道模型中所有的条件概率， 我们称之为模型的参数

通过对预料的统计， 得到这些参数的过程成为模型的训练

信息熵正是对不确定性的衡量

- 一个人要想在自己的领域做到世界一流，它的周围必须有非常多的一流人物。

BCJR算法 维特比算法

技术分两种 道 术

具体做事方法 是术

做事的原理和原则是 道

从独门绝技 到普及 在到落伍

追求术的人一辈子工作很辛苦 只有掌握了本质和精髓才能永远游刃有余

搜索 下载、索引和 排序

10^100 古高尔

在网络爬虫中， 人们使用一张离散表（哈希表）

区分深度 与 广度 看是不是一条路走到黑 还是 向四周发散

词频 停止词(的， 中， 地等)

如果一个查询包含n个关键词w1,w2,w3，。。。他们在一个特定网页中的词频分别是tf1,tf2,tf3,tf4。。。

tf1*idf1+tf2*idf2+tf12*idf2

有限状态机

伪随机数产生器算法

视频的匹配： 关键帧的提取和特征的提取

1.一个正确的数学模型应当在形式上是简单的

2.一个正确的模型一开始kneeing还不如一个精确习作过的错误模型来的准确，但是，我们认为大方向是正确的，就应该坚持下去

3.大量准确的数据对研发很重要

4，正确的模型也可能受噪音干扰，显得不准确；这时不应该用一种凑合的修正方法加以弥补，而是要找到噪音的根源

最大熵原理指出， 对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知条件，而对为止的情况不要做任何主观假设

这一代输入法的问题在于减少了每个汉字几件的次数，而忽视了找到每个键的事件。

帮助思维和记忆

对汉字的编码分为两部分： 对拼音的编码 和消除歧义性的编码

香农第一定理指出： 对于一个信息，任何编码的长度都不应小于它的信息熵

spam 垃圾邮件

切比雪夫不等式

数据成为决定搜索引擎好坏的第一要素， 而 算法倒在其次

网页点击的数据 用户的数据

算法的复杂度看0（）里面的函数变量的部分。

哈希算法 常熟复杂度

有序数组二分查找法 0log(N) 对数复杂度

图遍历 节点数n的线性复杂度